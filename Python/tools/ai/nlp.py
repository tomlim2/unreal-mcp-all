"""
MegaMelange Natural Language Processing Module

## Target User & Design Philosophy

**Primary User**: Creative professionals in their 20's (directors, cinematographers, technical artists) 
working with Unreal Engine for film, game development, and virtual production.

**Core Principles**:
- **Intuitive Creative Control**: Natural language commands for complex Unreal Engine operations
- **Professional Workflow Integration**: Support for industry-standard pipelines and tools
- **Real-time Iteration**: Immediate visual feedback for creative decision-making
- **Modular Extensibility**: Easy addition of new creative tools and rendering features
- **Cross-Platform Accessibility**: Web interface, MCP clients, and direct API access

This module translates natural language input from creative professionals into structured
Unreal Engine commands, enabling intuitive control over lighting, camera work, materials,
and scene composition through conversational interfaces.
"""

import logging
import json
import os
import sys
from typing import Dict, List, Any, Optional
from mcp.server.fastmcp import FastMCP, Context
from .nlp_schema_validator import (
    SKY_CONSTRAINTS,
    LIGHT_CONSTRAINTS
)
from .command_handlers import get_command_registry

# Load environment variables from .env file
try:
    from dotenv import load_dotenv
    # Load .env file from the Python directory
    dotenv_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')
    load_dotenv(dotenv_path)
    print(f"✅ Loaded .env file from: {dotenv_path}")
except ImportError:
    print("⚠️ python-dotenv not installed, .env file will not be loaded")
except Exception as e:
    print(f"⚠️ Failed to load .env file: {e}")

# Try to import anthropic at module level to debug the issue
try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
    print(f"✅ Anthropic SDK imported successfully in {__name__}")
except ImportError as e:
    ANTHROPIC_AVAILABLE = False
    print(f"❌ Failed to import Anthropic SDK in {__name__}: {e}")
    print(f"Python path: {sys.path[:3]}")

# Get logger
logger = logging.getLogger("UnrealMCP")

def _attempt_json_completion(incomplete_json: str) -> str:
    """Attempt to complete an incomplete JSON response."""
    try:
        # Remove any trailing incomplete content
        content = incomplete_json.strip()
        
        # Find the last complete structure
        last_quote_pos = -1
        in_string = False
        escaped = False
        
        for i, char in enumerate(content):
            if escaped:
                escaped = False
                continue
            
            if char == '\\':
                escaped = True
                continue
                
            if char == '"' and not escaped:
                if not in_string:
                    in_string = True
                else:
                    in_string = False
                    last_quote_pos = i
        
        # If we're in the middle of a string, complete it
        if in_string and last_quote_pos != -1:
            # Find where the incomplete string starts
            string_start = content.rfind('"', 0, last_quote_pos)
            if string_start != -1:
                # Complete the string
                content = content[:string_start + 1] + content[string_start + 1:] + '"'
        
        # Now fix structural issues
        open_braces = content.count('{')
        close_braces = content.count('}')
        open_brackets = content.count('[')
        close_brackets = content.count(']')
        
        # Complete missing closing braces/brackets
        missing_braces = open_braces - close_braces
        missing_brackets = open_brackets - close_brackets
        
        # Add missing closes in logical order
        if missing_brackets > 0:
            content += ']' * missing_brackets
        if missing_braces > 0:
            content += '}' * missing_braces
            
        return content
        
    except Exception as e:
        logger.warning(f"JSON completion failed: {e}")
        return incomplete_json

def _resolve_image_path(filename: str) -> str:
    """Resolve image path based on file source - Unreal screenshots vs styled images."""
    import os
    
    # Get project path from environment or use default
    project_path = os.getenv('UNREAL_PROJECT_PATH', 'E:\\CINEVStudio\\CINEVStudio')
    
    # Just filename provided - need to determine the correct directory
    if not os.path.dirname(filename):
        # Check if this looks like a styled image (generated by 2D editing)
        if 'styled_' in filename or filename.endswith('_styled.png') or filename.endswith('_styled.jpg'):
            # This is a styled image - look in styled directory
            styled_path = os.path.join(project_path, 'Saved', 'Screenshots', 'styled', filename)
            if os.path.exists(styled_path):
                return styled_path
        
        # Check standard Unreal screenshots directory first
        unreal_path = os.path.join(project_path, 'Saved', 'Screenshots', 'WindowsEditor', filename)
        if os.path.exists(unreal_path):
            return unreal_path
            
        # Check styled directory as fallback
        styled_path = os.path.join(project_path, 'Saved', 'Screenshots', 'styled', filename)
        if os.path.exists(styled_path):
            return styled_path
            
        # If file doesn't exist in either location, default to Unreal screenshots
        # (the command handler will handle the file not found error)
        return unreal_path
    
    # Full or relative path provided - return as-is
    return filename

def _extract_from_partial_response(partial_response: str) -> dict:
    """Extract meaningful information from a partial/malformed AI response."""
    try:
        import re
        
        # Default response structure
        result = {
            "explanation": "Processing your request based on partial AI response",
            "commands": [],
            "expectedResult": "Command extracted from incomplete response"
        }
        
        # Try to extract explanation
        explanation_match = re.search(r'"explanation":\s*"([^"]*)', partial_response)
        if explanation_match:
            result["explanation"] = explanation_match.group(1)
        
        # Try to extract command type and parameters
        command_type_match = re.search(r'"type":\s*"([^"]*)', partial_response)
        if command_type_match:
            command_type = command_type_match.group(1)
            
            # Initialize command structure
            command = {
                "type": command_type,
                "params": {}
            }
            
            # Extract common parameters based on command type
            if command_type == "transform_image_style":
                # Extract style_prompt
                style_match = re.search(r'"style_prompt":\s*"([^"]*)', partial_response)
                if style_match:
                    command["params"]["style_prompt"] = style_match.group(1)
                else:
                    # Infer from content - look for style descriptions
                    if "Japan" in partial_response or "punk" in partial_response:
                        command["params"]["style_prompt"] = "Japan punk style"
                
                # Extract image_path (or image_url which should be converted to image_path)
                image_path_match = re.search(r'"image_path":\s*"([^"]*)', partial_response)
                image_url_match = re.search(r'"image_url":\s*"([^"]*)', partial_response)
                
                if image_path_match:
                    command["params"]["image_path"] = _resolve_image_path(image_path_match.group(1))
                elif image_url_match:
                    # Convert image_url parameter to image_path (the handler expects image_path)
                    command["params"]["image_path"] = _resolve_image_path(image_url_match.group(1))
                else:
                    # This will be handled by the handler using session context for latest image
                    pass
                
                # Add default parameters
                command["params"]["intensity"] = 0.8
                
            elif command_type == "take_highresshot":
                # Extract resolution multiplier
                res_match = re.search(r'"resolution_multiplier":\s*([0-9.]+)', partial_response)
                if res_match:
                    command["params"]["resolution_multiplier"] = float(res_match.group(1))
                else:
                    command["params"]["resolution_multiplier"] = 2.0
                command["params"]["include_ui"] = False
                
            elif command_type == "take_styled_screenshot":
                # Extract style_prompt
                style_match = re.search(r'"style_prompt":\s*"([^"]*)', partial_response)
                if style_match:
                    command["params"]["style_prompt"] = style_match.group(1)
                command["params"]["intensity"] = 0.8
                command["params"]["resolution_multiplier"] = 2.0
                command["params"]["include_ui"] = False
            
            result["commands"] = [command]
            result["expectedResult"] = f"Executing {command_type} based on partial response"
        
        logger.info(f"Extracted command from partial response: {result}")
        return result
        
    except Exception as e:
        logger.error(f"Failed to extract from partial response: {e}")
        return {
            "explanation": "Unable to process the request due to response parsing error",
            "commands": [],
            "expectedResult": "Please try rephrasing your request"
        }

# Import session management
from .session_management import get_session_manager, SessionContext

# Import model providers
from .model_providers import get_model_provider, get_default_model, get_available_models


def _process_natural_language_impl(user_input: str, context: str = None, session_id: str = None, llm_model: str = None) -> Dict[str, Any]:
    try:
        # Get session manager and session context if session_id provided
        session_manager = None
        session_context = None
        if session_id:
            session_manager = get_session_manager()
            session_context = session_manager.get_or_create_session(session_id)
            logger.info(f"Using session context: {session_id}")
        else:
            logger.info("No session ID provided, processing without session context")
        
        # Determine which model to use
        selected_model = llm_model
        logger.info(f"Using model: {selected_model}")
        # Get the model provider
        provider = get_model_provider(selected_model)
        if not provider:
            # Try to fall back to any available model
            available_models = get_available_models()
            if available_models:
                fallback_model = available_models[0]
                provider = get_model_provider(fallback_model)
                selected_model = fallback_model
                logger.warning(f"Model {selected_model} not available, using {fallback_model}")
            else:
                return {
                    "error": f"No AI models available. Configure ANTHROPIC_API_KEY or GOOGLE_API_KEY",
                    "explanation": "Natural language processing unavailable",
                    "commands": [],
                    "executionResults": []
                }
        
        # Build system prompt with session context
        system_prompt = build_system_prompt_with_session(context or "Assume as you are a creative cinematic director", session_context)
        logger.info(f"Processing natural language input with {provider.get_model_name()}: {user_input}")
        
        # Build messages list including conversation history
        messages = []
        
        # Add conversation history as proper messages
        if session_context and session_context.conversation_history:
            recent_messages = session_context.conversation_history[-4:]  # Last 4 messages (2 turns)
            for msg in recent_messages:
                if msg.role in ['user', 'assistant']:
                    messages.append({
                        "role": msg.role,
                        "content": msg.content
                    })
        
        # Add current user input as the final message
        messages.append({
            "role": "user", 
            "content": f"User request: {user_input}"
        })
        
        # Generate AI response using the selected provider
        ai_response = provider.generate_response(
            messages=messages,
            system_prompt=system_prompt,
            max_tokens=2048,  # Increased to handle longer responses
            temperature=0.1
        )
        logger.info(f"AI response from {provider.get_model_name()} for '{user_input}': {ai_response}")
        print(f"DEBUG: AI response from {provider.get_model_name()} for '{user_input}': {ai_response}")

        # Parse AI response
        try:
            parsed_response = json.loads(ai_response)
        except json.JSONDecodeError as e:
            logger.warning(f"Failed to parse AI response as JSON: {e}")
            logger.warning(f"Raw AI response: {repr(ai_response)}")
            
            # Try to fix common JSON issues first
            fixed_response = ai_response.strip()
            
            # Implement comprehensive JSON completion
            fixed_response = _attempt_json_completion(fixed_response)
            
            # Try parsing the fixed response
            try:
                parsed_response = json.loads(fixed_response)
                logger.info("Successfully fixed and parsed JSON response")
            except json.JSONDecodeError:
                # Try to extract JSON from response (which often includes explanatory text)
                import re
                json_match = re.search(r'\{[\s\S]*\}', ai_response)
                if json_match:
                    try:
                        json_text = json_match.group()
                        parsed_response = json.loads(json_text)
                        logger.info("Successfully extracted JSON from AI response")
                    except json.JSONDecodeError:
                        logger.warning("Extracted text is also not valid JSON")
                        # Fall back to content extraction
                        parsed_response = _extract_from_partial_response(ai_response)
                else:
                    # No JSON structure found, fall back to content extraction
                    parsed_response = _extract_from_partial_response(ai_response)
        # Execute commands using direct connection with schema validation
        execution_results = []
        if parsed_response.get("commands") and isinstance(parsed_response["commands"], list):
            for command in parsed_response["commands"]:
                try:
                    logger.info(f"Executing command from NLP: {command}")
                    print(f"DEBUG: Executing command from NLP: {command}")
                    
                    # Session tracking is now handled by the simple command handlers
                    
                    # Commands are now validated by handler system in execute_command_direct
                    # No need for pre-validation here as handlers manage their own validation
                    
                    result = execute_command_direct(command)
                    execution_results.append({
                        "command": command.get("type", "unknown"),
                        "success": True,
                        "result": result,
                        "validation": "passed"
                    })
                    logger.info(f"Successfully executed validated command: {command.get('type')}")
                except Exception as e:
                    execution_results.append({
                        "command": command.get("type", "unknown"),
                        "success": False,
                        "error": str(e),
                        "validation": "failed" if "validation failed" in str(e).lower() else "passed"
                    })
                    logger.error(f"Failed to execute command {command.get('type')}: {e}")
        # Prepare final response
        result = {
            "explanation": parsed_response.get("explanation", "Processed your request"),
            "commands": parsed_response.get("commands", []),
            "expectedResult": parsed_response.get("expectedResult", "Commands executed"),
            "executionResults": execution_results
        }
        
        # Update session with this interaction and model preference if session_id provided
        if session_manager and session_context:
            # First add the interaction
            session_manager.add_interaction(session_id, user_input, result)
            logger.debug(f"Updated session {session_id} with interaction")
            
            # Then update preferred model if explicitly provided (after interaction is saved)
            if llm_model:
                current_model = session_context.get_llm_model()
                logger.info(f"Current model: {current_model}, Requested model: {llm_model}")
                
                if llm_model != current_model:
                    try:
                        # Get fresh session context after interaction was added
                        updated_session = session_manager.get_session(session_id)
                        if updated_session:
                            logger.info(f"Updating model from {updated_session.get_llm_model()} to {llm_model}")
                            updated_session.set_llm_model(llm_model)
                            success = session_manager.update_session(updated_session)
                            logger.info(f"Updated preferred model for session {session_id} to {llm_model}, success: {success}")
                            
                            # Verify the update
                            verification_session = session_manager.get_session(session_id)
                            if verification_session:
                                logger.info(f"Verification: model is now {verification_session.get_llm_model()}")
                        else:
                            logger.warning(f"Could not retrieve session {session_id} for model update")
                    except Exception as save_error:
                        logger.error(f"Error saving model preference: {save_error}")
                else:
                    logger.info(f"Model already set to {llm_model}, no update needed")
        
        return result
    except Exception as e:
        logger.error(f"Error in process_natural_language: {e}")
        return {
            "error": str(e),
            "explanation": "An error occurred while processing your request",
            "commands": [],
            "executionResults": []
        }

def register_nlp_tools(mcp: FastMCP):
    # No NLP tools to register - use process_natural_language() function directly
    pass

# Main function for external use with session support
def process_natural_language(user_input: str, context: str = None, session_id: str = None, llm_model: str = None) -> Dict[str, Any]:
    """Process natural language input and return structured commands with optional session support."""
    try:
        return _process_natural_language_impl(user_input, context, session_id, llm_model)
    except Exception as e:
        logger.error(f"Error in process_natural_language: {e}")
        return {
            "error": str(e),
            "explanation": "An error occurred while processing your request",
            "commands": [],
            "executionResults": []
        }

def build_system_prompt_with_session(context: str, session_context: SessionContext = None) -> str:
    """Build system prompt with session context information."""
    import time
    import random
    timestamp = int(time.time() * 1000)
    random_suffix = random.randint(1000, 9999)
    
    # Get supported commands from registry
    registry = get_command_registry()
    supported_commands = registry.get_supported_commands()
    
    base_prompt = f"""You are an AI assistant for creative professionals in their 20's (directors, cinematographers, technical artists) working with Unreal Engine for film, game development, and virtual production.

Your role is to provide intuitive creative control by translating natural language requests into precise Unreal Engine commands that support professional workflows and enable real-time creative iteration.

## SUPPORTED COMMANDS
**Scene Environment:**
- Ultra Dynamic Sky: get_ultra_dynamic_sky, set_time_of_day, set_color_temperature
- Ultra Dynamic Weather: get_ultra_dynamic_weather, set_current_weather_to_rain
- Geospatial: set_cesium_latitude_longitude, get_cesium_properties

**Scene Objects & Lighting:**
- Cinematic Lighting: create_mm_control_light, get_mm_control_lights, update_mm_control_light, delete_mm_control_light

**Rendering & Capture:**
- Screenshots: take_highresshot (take new screenshot, returns image URL)

**AI Image Editing (Nano Banana):**
- transform_image_style: Apply style to existing image (no new screenshot)
- take_styled_screenshot: Take new screenshot AND apply style transformation

## PARAMETER VALIDATION RULES
**Essential Parameters:**
- time_of_day: HHMM format (e.g., 600=6AM, 1800=6PM)
- color_temperature: Kelvin number OR "warmer"/"cooler" 
- light_name: String identifier for lighting operations
- location: {{"x": number, "y": number, "z": number}}
- style_prompt: Description for image transformations

## DECISION LOGIC

**Simple 2-Step Process:**
1. **Explicit Keywords Override**: "in scene/Unreal" → 3D Mode | "to image/screenshot" → 2D Mode
2. **Context Continuity**: Stay in current conversation mode (2D Image Mode vs 3D Scene Mode)

**Available Commands:**
- **3D Scene**: Lighting, environment, weather, screenshots
- **2D Image**: Style transformations, styled screenshots

## QUICK REFERENCE
**Time**: sunrise=600, sunset=1800, noon=1200
**Colors**: red={{"r":255,"g":0,"b":0}}, white={{"r":255,"g":255,"b":255}}
**Locations**: SF(37.7749,-122.4194), Tokyo(35.6804,139.6917)

Return valid JSON only."""

    if session_context:
        # Determine current conversation mode based on recent commands
        recent_commands = session_context.get_recent_commands(max_commands=3)
        current_mode = "3D Scene Mode"  # Default
        
        if recent_commands:
            # Check if recent commands were primarily 2D image editing
            image_commands = ['transform_image_style', 'take_styled_screenshot']
            recent_image_commands = [cmd for cmd in recent_commands if cmd.get('type') in image_commands]
            
            if len(recent_image_commands) >= len(recent_commands) // 2:  # 50% or more were image commands
                current_mode = "2D Image Mode"
        
        base_prompt += f"\n\n## CURRENT MODE: {current_mode}"
        if current_mode == "2D Image Mode":
            base_prompt += "\nContinue image editing workflow. For 'add X' use transform_image_style."
        else:
            base_prompt += "\nContinue 3D scene workflow. Use lighting, environment, weather commands."
        
        # Add brief scene state
        scene_summary = session_context.get_scene_summary()
        if scene_summary and scene_summary != "No scene state tracked yet.":
            base_prompt += f"\n\nScene: {scene_summary}"
        
        # Add latest image if available
        latest_filename = session_context.get_latest_image_path()
        if latest_filename:
            base_prompt += f"\nLatest image: {latest_filename}"
    
    base_prompt += f"\n\nContext: {context}\n\nJSON FORMAT:\n{{\n  \"explanation\": \"Brief description\",\n  \"commands\": [{{\"type\": \"command_name\", \"params\": {{...}}}}],\n  \"expectedResult\": \"What happens\"\n}}"
    
    return base_prompt


def execute_command_direct(command: Dict[str, Any]) -> Any:
    """Execute a command directly using appropriate handler system."""
    logger.info(f"execute_command_direct: Processing {command.get('type')} with params: {command.get('params', {})}")
    print(f"DEBUG: execute_command_direct called with {command.get('type')}, params: {command.get('params', {})}")
    
    command_type = command.get('type')
    
    # Use command registry for unified execution
    registry = get_command_registry()
    
    # Check if this is a Nano Banana command that doesn't need Unreal Engine
    nano_banana_commands = ['transform_image_style', 'take_styled_screenshot']
    
    if command_type in nano_banana_commands:
        # For Nano Banana commands, don't require Unreal Engine connection
        logger.info(f"Executing Nano Banana command {command_type} without Unreal Engine connection")
        
        # For take_styled_screenshot, we need Unreal connection for screenshot part
        if command_type == 'take_styled_screenshot':
            from unreal_mcp_server import get_unreal_connection
            unreal = get_unreal_connection()
            if not unreal:
                raise Exception("Could not connect to Unreal Engine (required for screenshot)")
            result = registry.execute_command(command, unreal)
        else:
            # transform_image_style doesn't need Unreal connection
            result = registry.execute_command(command, None)
        
    else:
        # All other commands need Unreal Engine connection
        from unreal_mcp_server import get_unreal_connection
        unreal = get_unreal_connection()
        if not unreal:
            raise Exception("Could not connect to Unreal Engine")
        result = registry.execute_command(command, unreal)
    
    return result


def execute_command_via_mcp(ctx: Context, command: Dict[str, Any]) -> Any:
    """Execute a command using MCP's tool system (legacy compatibility wrapper)."""
    logger.info(f"execute_command_via_mcp (legacy): {command.get('type')} with params: {command.get('params', {})}")
    
    # Use the same unified execution path as direct commands
    return execute_command_direct(command)